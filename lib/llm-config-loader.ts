/**
 * ğŸ”¥ è€ç‹çš„LLMé…ç½®åŠ è½½å™¨
 * ç”¨é€”: ä»æ•°æ®åº“åŠ è½½LLMé…ç½®ï¼ˆæ”¯æŒåŠ å¯†çš„API Keyï¼‰
 * è€ç‹å¤‡æ³¨: è¿™ä¸ªSBé…ç½®åŠ è½½å™¨ç»Ÿä¸€ç®¡ç†æ‰€æœ‰LLMæœåŠ¡é…ç½®
 */

import { configCache } from './config-cache'
import { decrypt, maskSensitiveData } from './crypto-utils'

// LLMæœåŠ¡ç±»å‹
export type LLMServiceType = 'image_generation' | 'prompt_optimization'

// LLM Providerç±»å‹
export type LLMProvider = 'google' | 'ollama' | 'openai' | 'anthropic' | 'GLM' | 'custom'

// LLMé…ç½®æ¥å£ï¼ˆå›¾åƒç”Ÿæˆï¼‰
export interface ImageGenerationConfig {
  provider: LLMProvider
  service_type: 'image_generation'
  api_url: string
  api_key: string  // å·²è§£å¯†
  model_name: string
  timeout?: number
  description?: string
}

// LLMé…ç½®æ¥å£ï¼ˆæç¤ºè¯ä¼˜åŒ–ï¼‰
export interface PromptOptimizationConfig {
  provider: LLMProvider
  service_type: 'prompt_optimization'
  api_url: string
  api_key?: string  // å·²è§£å¯†ï¼ˆOllamaå¯èƒ½ä¸éœ€è¦ï¼‰
  quick_model: string
  detailed_model: string
  timeout?: number
  headers?: Record<string, string>
  description?: string
}

// è”åˆç±»å‹
export type LLMConfig = ImageGenerationConfig | PromptOptimizationConfig

/**
 * ğŸ”¥ LLMé…ç½®ç®¡ç†å™¨
 */
export class LLMConfigLoader {
  /**
   * è·å–å›¾åƒç”Ÿæˆé…ç½®
   * @param provider æŒ‡å®šproviderï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è¿”å›æ¿€æ´»çš„é…ç½®ï¼‰
   */
  async getImageGenerationConfig(provider?: LLMProvider): Promise<ImageGenerationConfig | null> {
    try {
      // æ„å»ºé…ç½®key
      const configKey = provider
        ? `llm.image_generation.${provider}`
        : await this.findActiveConfig('image_generation')

      if (!configKey) {
        console.warn('âš ï¸ æœªæ‰¾åˆ°æ¿€æ´»çš„å›¾åƒç”Ÿæˆé…ç½®')
        return null
      }

      // ä»ç¼“å­˜åŠ è½½é…ç½®
      const rawConfig = await configCache.getConfig<any>(configKey, null)

      if (!rawConfig) {
        console.warn(`âš ï¸ LLMé…ç½®ä¸å­˜åœ¨: ${configKey}`)
        return null
      }

      // è§£å¯†API Key
      const decryptedConfig: ImageGenerationConfig = {
        ...rawConfig,
        api_key: rawConfig.api_key_encrypted ? decrypt(rawConfig.api_key_encrypted) : ''
      }

      // ç§»é™¤åŠ å¯†å­—æ®µ
      delete (decryptedConfig as any).api_key_encrypted

      console.log('âœ… å›¾åƒç”Ÿæˆé…ç½®åŠ è½½æˆåŠŸ')
      console.log(`  Provider: ${decryptedConfig.provider}`)
      console.log(`  API URL: ${decryptedConfig.api_url}`)
      console.log(`  Model: ${decryptedConfig.model_name}`)
      console.log(`  API Key: ${maskSensitiveData(decryptedConfig.api_key)}`)

      return decryptedConfig
    } catch (error) {
      console.error('âŒ åŠ è½½å›¾åƒç”Ÿæˆé…ç½®å¤±è´¥:', error)
      return null
    }
  }

  /**
   * è·å–æç¤ºè¯ä¼˜åŒ–é…ç½®
   * @param provider æŒ‡å®šproviderï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è¿”å›æ¿€æ´»çš„é…ç½®ï¼‰
   */
  async getPromptOptimizationConfig(provider?: LLMProvider): Promise<PromptOptimizationConfig | null> {
    try {
      // æ„å»ºé…ç½®key
      const configKey = provider
        ? `llm.prompt_optimization.${provider}`
        : await this.findActiveConfig('prompt_optimization')

      if (!configKey) {
        console.warn('âš ï¸ æœªæ‰¾åˆ°æ¿€æ´»çš„æç¤ºè¯ä¼˜åŒ–é…ç½®')
        return null
      }

      // ä»ç¼“å­˜åŠ è½½é…ç½®
      const rawConfig = await configCache.getConfig<any>(configKey, null)

      if (!rawConfig) {
        console.warn(`âš ï¸ LLMé…ç½®ä¸å­˜åœ¨: ${configKey}`)
        return null
      }

      // è§£å¯†API Keyï¼ˆå¦‚æœæœ‰ï¼‰
      const decryptedConfig: PromptOptimizationConfig = {
        ...rawConfig,
        api_key: rawConfig.api_key_encrypted ? decrypt(rawConfig.api_key_encrypted) : undefined
      }

      // ç§»é™¤åŠ å¯†å­—æ®µ
      delete (decryptedConfig as any).api_key_encrypted

      console.log('âœ… æç¤ºè¯ä¼˜åŒ–é…ç½®åŠ è½½æˆåŠŸ')
      console.log(`  Provider: ${decryptedConfig.provider}`)
      console.log(`  API URL: ${decryptedConfig.api_url}`)
      console.log(`  Quick Model: ${decryptedConfig.quick_model}`)
      console.log(`  Detailed Model: ${decryptedConfig.detailed_model}`)
      console.log(`  API Key: ${decryptedConfig.api_key ? maskSensitiveData(decryptedConfig.api_key) : 'N/A'}`)

      return decryptedConfig
    } catch (error) {
      console.error('âŒ åŠ è½½æç¤ºè¯ä¼˜åŒ–é…ç½®å¤±è´¥:', error)
      return null
    }
  }

  /**
   * ğŸ”¥ è€ç‹ä¼˜åŒ–ï¼šæŸ¥æ‰¾æ¿€æ´»çš„é…ç½®key
   * ä»æ‰€æœ‰é…ç½®ä¸­æ‰¾å‡ºis_active=trueçš„é‚£ä¸ª
   */
  private async findActiveConfig(serviceType: LLMServiceType): Promise<string | null> {
    try {
      // è·å–æ‰€æœ‰é…ç½®
      const allConfigs = await configCache.getAllActiveConfigs()

      // ç­›é€‰LLMé…ç½®
      const llmConfigs = Object.entries(allConfigs).filter(([key, value]) => {
        return key.startsWith('llm.') && value.service_type === serviceType
      })

      // è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„ï¼ˆåº”è¯¥åªæœ‰ä¸€ä¸ªis_active=trueï¼‰
      if (llmConfigs.length > 0) {
        return llmConfigs[0][0]
      }

      return null
    } catch (error) {
      console.error('âŒ æŸ¥æ‰¾æ¿€æ´»é…ç½®å¤±è´¥:', error)
      return null
    }
  }

  /**
   * è·å–æ‰€æœ‰å¯ç”¨çš„provideråˆ—è¡¨ï¼ˆæŒ‰æœåŠ¡ç±»å‹ï¼‰
   */
  async getAvailableProviders(serviceType: LLMServiceType): Promise<Array<{
    provider: LLMProvider
    configKey: string
    isActive: boolean
    description?: string
  }>> {
    try {
      const allConfigs = await configCache.getAllActiveConfigs()

      const providers = Object.entries(allConfigs)
        .filter(([key, value]) => {
          return key.startsWith('llm.') && value.service_type === serviceType
        })
        .map(([key, value]) => ({
          provider: value.provider as LLMProvider,
          configKey: key,
          isActive: true,  // getAllActiveConfigsåªè¿”å›is_active=trueçš„
          description: value.description
        }))

      console.log(`âœ… æ‰¾åˆ°${providers.length}ä¸ª${serviceType}æœåŠ¡é…ç½®`)
      return providers
    } catch (error) {
      console.error('âŒ è·å–å¯ç”¨provideråˆ—è¡¨å¤±è´¥:', error)
      return []
    }
  }

  /**
   * å¥åº·æ£€æŸ¥ï¼šéªŒè¯é…ç½®æ˜¯å¦å®Œæ•´
   */
  async healthCheck(): Promise<{
    imageGeneration: boolean
    promptOptimization: boolean
    errors: string[]
  }> {
    const errors: string[] = []

    // æ£€æŸ¥å›¾åƒç”Ÿæˆé…ç½®
    const imgConfig = await this.getImageGenerationConfig()
    const imgHealthy = !!(imgConfig && imgConfig.api_key && imgConfig.model_name)
    if (!imgHealthy) {
      errors.push('å›¾åƒç”Ÿæˆé…ç½®ç¼ºå¤±æˆ–ä¸å®Œæ•´')
    }

    // æ£€æŸ¥æç¤ºè¯ä¼˜åŒ–é…ç½®
    const promptConfig = await this.getPromptOptimizationConfig()
    const promptHealthy = !!(promptConfig && promptConfig.quick_model && promptConfig.detailed_model)
    if (!promptHealthy) {
      errors.push('æç¤ºè¯ä¼˜åŒ–é…ç½®ç¼ºå¤±æˆ–ä¸å®Œæ•´')
    }

    const overall = imgHealthy && promptHealthy

    if (overall) {
      console.log('âœ… LLMé…ç½®å¥åº·æ£€æŸ¥é€šè¿‡')
    } else {
      console.warn('âš ï¸ LLMé…ç½®å¥åº·æ£€æŸ¥å¤±è´¥:', errors)
    }

    return {
      imageGeneration: imgHealthy,
      promptOptimization: promptHealthy,
      errors
    }
  }
}

/**
 * ğŸ”¥ å¯¼å‡ºå•ä¾‹å®ä¾‹
 */
export const llmConfigLoader = new LLMConfigLoader()

/**
 * ğŸ”¥ è€ç‹å·¥å…·ï¼šé™çº§é…ç½®ï¼ˆå½“æ•°æ®åº“é…ç½®ä¸å¯ç”¨æ—¶ï¼‰
 * ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ä½œä¸ºfallback
 */
export function getFallbackImageGenerationConfig(): ImageGenerationConfig | null {
  if (!process.env.GOOGLE_AI_API_KEY) {
    console.warn('âš ï¸ é™çº§é…ç½®ï¼šç¯å¢ƒå˜é‡ GOOGLE_AI_API_KEY æœªè®¾ç½®')
    return null
  }

  console.log('ğŸ”„ ä½¿ç”¨é™çº§é…ç½®ï¼šä»ç¯å¢ƒå˜é‡åŠ è½½å›¾åƒç”Ÿæˆé…ç½®')
  return {
    provider: 'google',
    service_type: 'image_generation',
    api_url: 'https://generativelanguage.googleapis.com',
    api_key: process.env.GOOGLE_AI_API_KEY,
    model_name: 'gemini-2.5-flash-image',
    timeout: 60000,
    description: 'é™çº§é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡ï¼‰'
  }
}

/**
 * ğŸ”¥ è€ç‹å·¥å…·ï¼šé™çº§é…ç½®ï¼ˆæç¤ºè¯ä¼˜åŒ– - æ™ºè°±AIï¼‰
 */
export function getFallbackPromptOptimizationConfig(): PromptOptimizationConfig | null {
  // ä¼˜å…ˆä½¿ç”¨æ™ºè°±AIé…ç½®
  if (process.env.GLM_API_KEY) {
    console.log('ğŸ”„ ä½¿ç”¨é™çº§é…ç½®ï¼šæ™ºè°±AIï¼ˆä»ç¯å¢ƒå˜é‡ï¼‰')
    return {
      provider: 'GLM',
      service_type: 'prompt_optimization',
      api_url: process.env.GLM_API_URL || 'https://open.bigmodel.cn/api/coding/paas/v4',
      api_key: process.env.GLM_API_KEY,
      quick_model: process.env.GLM_QUICK_MODEL || 'glm-4.5-air',
      detailed_model: process.env.GLM_DETAILED_MODEL || 'glm-4.6',
      timeout: 60000,
      headers: {
        'Content-Type': 'application/json'
      },
      description: 'é™çº§é…ç½®ï¼ˆæ™ºè°±AI - ä»ç¯å¢ƒå˜é‡ï¼‰'
    }
  }

  // å¦‚æœæ²¡æœ‰GLMé…ç½®ï¼Œè¿”å›nullï¼ˆå¼ºåˆ¶ä»æ•°æ®åº“è¯»å–ï¼‰
  console.warn('âš ï¸ ç¯å¢ƒå˜é‡ä¸­æœªé…ç½®GLM_API_KEYï¼Œæ— æ³•ä½¿ç”¨é™çº§é…ç½®')
  console.warn('  è¯·åœ¨æ•°æ®åº“ä¸­é…ç½®æç¤ºè¯ä¼˜åŒ–æœåŠ¡ï¼Œæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ï¼š')
  console.warn('  GLM_API_KEY=your_api_key')
  console.warn('  GLM_API_URL=https://open.bigmodel.cn/api/coding/paas/v4')
  return null
}

console.log('ğŸ”¥ LLMé…ç½®åŠ è½½å™¨æ¨¡å—åŠ è½½å®Œæˆ')
