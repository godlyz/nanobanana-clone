/**
 * ğŸ”¥ è€ç‹é‡æ„ï¼šé€šç”¨LLMæç¤ºè¯ä¼˜åŒ–å™¨
 *
 * åŠŸèƒ½ï¼š
 * - æ”¯æŒå¤šç§LLMæä¾›å•†ï¼ˆOllamaã€OpenAIã€Anthropicç­‰ï¼‰
 * - æ”¯æŒå¿«é€Ÿ/è¯¦ç»†ä¸¤ç§ä¼˜åŒ–æ¨¡å¼
 * - åˆ†ææç¤ºè¯è´¨é‡ï¼ˆå®Œæ•´æ€§ã€æ¸…æ™°åº¦ã€åˆ›æ„æ€§ã€å…·ä½“æ€§ï¼‰
 * - ç”Ÿæˆç»“æ„åŒ–ä¼˜åŒ–å»ºè®®å’Œå¤šä¸ªå¤‡é€‰æ–¹æ¡ˆ
 * - å†…ç½®é™çº§å¤„ç†ï¼Œç¡®ä¿ç³»ç»Ÿå¯ç”¨æ€§
 * - ğŸ”¥ è€ç‹é‡æ„ï¼šé…ç½®ä¼˜å…ˆçº§ï¼šæ•°æ®åº“ > ç¯å¢ƒå˜é‡
 */

import { Ollama } from 'ollama'
import * as fs from 'fs'
import * as path from 'path'
import { llmConfigLoader, getFallbackPromptOptimizationConfig } from './llm-config-loader' // ğŸ”¥ è€ç‹æ–°å¢ï¼šä»æ•°æ®åº“åŠ è½½é…ç½®

// ğŸ”¥ è€ç‹æ·»åŠ ï¼šLLMé…ç½®æ¥å£
interface LLMConfig {
  provider: 'ollama' | 'openai' | 'anthropic' | 'GLM' | 'custom'
  apiUrl: string
  apiKey?: string
  models: {
    quick: string
    detailed: string
  }
  timeout?: number
  headers?: Record<string, string>
}

interface OptimizationOptions {
  level: 'quick' | 'detailed'
  category?: string
  userPreferences?: {
    preferredStyle?: string
    preferredLighting?: string
    preferredComposition?: string
    translateToEnglish?: boolean  // ğŸ”¥ è€ç‹æ·»åŠ ï¼šæ˜¯å¦ç¿»è¯‘æˆè‹±æ–‡
  }
}

interface OptimizationResult {
  selected: {
    optimizedPrompt: string
    improvements: string[]
    qualityScore: number
  }
  analysis: {
    completeness: number
    clarity: number
    creativity: number
    specificity: number
    overallScore: number
    weaknesses: string[]
    suggestions: string[]
  }
  costEstimate: {
    optimizationCost: number
    potentialBenefit: string
    roi: string
  }
  alternatives: Array<{
    optimizedPrompt: string
    improvements: string[]
    qualityScore: number
  }>
}

export class OllamaPromptOptimizer {
  private config!: LLMConfig  // ğŸ”¥ è€ç‹é‡æ„ï¼šä½¿ç”¨éç©ºæ–­è¨€ï¼Œå»¶è¿Ÿåˆå§‹åŒ–
  private ollama?: Ollama  // ğŸ”¥ è€ç‹æ”¹é€ ï¼šåªåœ¨provider=ollamaæ—¶ä½¿ç”¨
  private configLoaded: boolean = false  // ğŸ”¥ è€ç‹æ–°å¢ï¼šé…ç½®åŠ è½½çŠ¶æ€æ ‡å¿—
  private configPromise: Promise<void> | null = null  // ğŸ”¥ è€ç‹æ–°å¢ï¼šé˜²æ­¢å¹¶å‘åŠ è½½

  constructor() {
    // ğŸ”¥ è€ç‹é‡æ„ï¼šæ„é€ å‡½æ•°ä¸å†åŒæ­¥åŠ è½½é…ç½®ï¼Œæ”¹ä¸ºlazy initialization
    console.log('ğŸ”§ LLM Optimizeråˆå§‹åŒ–ï¼ˆé…ç½®å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åŠ è½½ï¼‰')
  }

  /**
   * ğŸ”¥ è€ç‹é‡æ„ï¼šå¼‚æ­¥åŠ è½½LLMé…ç½®ï¼ˆæ”¯æŒæ•°æ®åº“ï¼‰
   * é…ç½®ä¼˜å…ˆçº§ï¼šæ•°æ®åº“ > ç¯å¢ƒå˜é‡
   * ä½¿ç”¨lazy initializationæ¨¡å¼ï¼Œé¿å…æ„é€ å‡½æ•°asyncé—®é¢˜
   */
  private async ensureConfigLoaded(): Promise<void> {
    // å¦‚æœå·²ç»åŠ è½½å®Œæˆï¼Œç›´æ¥è¿”å›
    if (this.configLoaded) {
      return
    }

    // å¦‚æœæ­£åœ¨åŠ è½½ä¸­ï¼Œç­‰å¾…åŠ è½½å®Œæˆï¼ˆé˜²æ­¢å¹¶å‘è°ƒç”¨ï¼‰
    if (this.configPromise) {
      return this.configPromise
    }

    // å¼€å§‹åŠ è½½é…ç½®
    this.configPromise = (async () => {
      try {
        console.log('ğŸ” æ­£åœ¨ä»æ•°æ®åº“åŠ è½½æç¤ºè¯ä¼˜åŒ–é…ç½®...')

        // ğŸ”¥ æ–¹å¼1ï¼šä»æ•°æ®åº“åŠ è½½é…ç½®ï¼ˆæ¨èï¼‰
        const dbConfig = await llmConfigLoader.getPromptOptimizationConfig()

        if (dbConfig) {
          console.log('âœ… ä»æ•°æ®åº“åŠ è½½é…ç½®æˆåŠŸ')
          // è½¬æ¢ä¸ºå†…éƒ¨LLMConfigæ ¼å¼
          this.config = {
            provider: dbConfig.provider as any,  // LLMProviderç±»å‹å…¼å®¹
            apiUrl: dbConfig.api_url,
            apiKey: dbConfig.api_key,
            models: {
              quick: dbConfig.quick_model,
              detailed: dbConfig.detailed_model
            },
            timeout: dbConfig.timeout || 60000,
            headers: dbConfig.headers
          }
        } else {
          // ğŸ”¥ æ–¹å¼2ï¼šé™çº§åˆ°ç¯å¢ƒå˜é‡é…ç½®
          console.warn('âš ï¸ æ•°æ®åº“é…ç½®ä¸å¯ç”¨ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡é™çº§é…ç½®')
          const fallbackConfig = getFallbackPromptOptimizationConfig()

          if (!fallbackConfig) {
            throw new Error('æ— æ³•åŠ è½½æç¤ºè¯ä¼˜åŒ–é…ç½®ï¼šæ•°æ®åº“å’Œç¯å¢ƒå˜é‡éƒ½ä¸å¯ç”¨')
          }

          this.config = {
            provider: fallbackConfig.provider as any,
            apiUrl: fallbackConfig.api_url,
            apiKey: fallbackConfig.api_key,
            models: {
              quick: fallbackConfig.quick_model,
              detailed: fallbackConfig.detailed_model
            },
            timeout: fallbackConfig.timeout || 60000,
            headers: fallbackConfig.headers
          }

          console.log('âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡é™çº§é…ç½®')
        }

        // ğŸ”¥ è€ç‹åˆå§‹åŒ–ï¼šæ ¹æ®providerç±»å‹åˆå§‹åŒ–å¯¹åº”çš„å®¢æˆ·ç«¯
        if (this.config.provider === 'ollama') {
          this.ollama = new Ollama({
            host: this.config.apiUrl
          })
          console.log('âœ… Ollamaå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ')
        }

        console.log('ğŸ”§ LLM Optimizeré…ç½®åŠ è½½å®Œæˆ:')
        console.log('  Provider:', this.config.provider)
        console.log('  API URL:', this.config.apiUrl)
        console.log('  å¿«é€Ÿæ¨¡å¼æ¨¡å‹:', this.config.models.quick)
        console.log('  è¯¦ç»†æ¨¡å¼æ¨¡å‹:', this.config.models.detailed)
        console.log('  API Key:', this.config.apiKey ? 'å·²é…ç½® âœ“' : 'æœªé…ç½® âœ—')

        this.configLoaded = true
      } catch (error) {
        console.error('âŒ åŠ è½½æç¤ºè¯ä¼˜åŒ–é…ç½®å¤±è´¥:', error)
        this.configPromise = null  // é‡ç½®Promiseï¼Œå…è®¸é‡è¯•
        throw error
      }
    })()

    return this.configPromise
  }

  /**
   * ğŸ”¥ è€ç‹ä¿ç•™ï¼šæ—§çš„åŒæ­¥åŠ è½½æ–¹æ³•ï¼ˆå·²åºŸå¼ƒï¼Œä»…ä½œä¸ºfallbackå‚è€ƒï¼‰
   * æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¸å†ä½¿ç”¨ï¼Œä¿ç•™ä»…ä¸ºå‘åå…¼å®¹
   */
  private loadConfig(): LLMConfig {
    // ğŸ”¥ æ–¹å¼1ï¼šå°è¯•ä» llm-config.json è¯»å–
    try {
      const configPath = path.join(process.cwd(), 'llm-config.json')

      if (fs.existsSync(configPath)) {
        const configFile = fs.readFileSync(configPath, 'utf-8')
        const config = JSON.parse(configFile) as LLMConfig

        console.log('âœ… ä» llm-config.json åŠ è½½é…ç½®æˆåŠŸ')
        return config
      }
    } catch (error) {
      console.warn('âš ï¸ è¯»å– llm-config.json å¤±è´¥ï¼Œå›é€€åˆ°ç¯å¢ƒå˜é‡')
      console.warn('é”™è¯¯:', error instanceof Error ? error.message : String(error))
    }

    // ğŸ”¥ æ–¹å¼2ï¼šä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆå…¼å®¹æ—§é…ç½®ï¼‰
    console.log('ğŸ“ ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®')
    return {
      provider: 'ollama',
      apiUrl: process.env.OLLAMA_API_URL || 'https://ollama.com/api',
      apiKey: process.env.OLLAMA_API_KEY,
      models: {
        quick: process.env.OLLAMA_QUICK_MODEL || 'gpt-0ss:120b-cloud',
        detailed: process.env.OLLAMA_DETAILED_MODEL || 'deepseek-r1:671b'
      },
      timeout: 60000
    }
  }

  /**
   * ğŸ”¥ è€ç‹è®¾è®¡ï¼šæ„å»ºä¼˜åŒ–æç¤ºè¯çš„system prompt
   *
   * æ ¹æ®ä¸åŒçš„ä¼˜åŒ–æ¨¡å¼ã€ç±»åˆ«å’Œç”¨æˆ·åå¥½ï¼ŒåŠ¨æ€ç”Ÿæˆæœ€ä¼˜çš„system prompt
   */
  private buildSystemPrompt(options: OptimizationOptions): string {
    const { level, category, userPreferences } = options

    // ğŸ”¥ è€ç‹æ·»åŠ ï¼šæ ¹æ®ç”¨æˆ·è®¾ç½®å†³å®šè¾“å‡ºè¯­è¨€
    const translateToEnglish = userPreferences?.translateToEnglish || false
    const languageInstruction = translateToEnglish
      ? '\n\n**è¯­è¨€è¦æ±‚ï¼šç»Ÿä¸€ä½¿ç”¨è‹±æ–‡è¾“å‡ºä¼˜åŒ–åçš„æç¤ºè¯ï¼ˆoptimizedPromptå­—æ®µå¿…é¡»æ˜¯è‹±æ–‡ï¼‰ã€‚**'
      : '\n\n**è¯­è¨€è¦æ±‚ï¼šä¿æŒè¾“å…¥è¯­è¨€å’Œè¾“å‡ºè¯­è¨€ä¸€è‡´ã€‚å¦‚æœç”¨æˆ·è¾“å…¥ä¸­æ–‡ï¼ŒoptimizedPromptå¿…é¡»ç”¨ä¸­æ–‡ï¼›å¦‚æœè¾“å…¥è‹±æ–‡ï¼ŒoptimizedPromptå¿…é¡»ç”¨è‹±æ–‡ã€‚ä¸è¦æ“…è‡ªç¿»è¯‘ï¼**'

    let systemPrompt = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå›¾åƒç”Ÿæˆæç¤ºè¯ä¼˜åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·çš„æç¤ºè¯å¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚

**åˆ†æç»´åº¦è¯´æ˜ï¼š**
1. **å®Œæ•´æ€§ (completeness, 0-100åˆ†)**: æç¤ºè¯æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ç»†èŠ‚ï¼ˆä¸»ä½“ã€åœºæ™¯ã€æ°›å›´ã€é£æ ¼ç­‰ï¼‰
2. **æ¸…æ™°åº¦ (clarity, 0-100åˆ†)**: æè¿°æ˜¯å¦æ¸…æ™°æ˜ç¡®ï¼Œæ²¡æœ‰æ­§ä¹‰
3. **åˆ›æ„æ€§ (creativity, 0-100åˆ†)**: æ˜¯å¦æœ‰ç‹¬ç‰¹çš„åˆ›æ„å…ƒç´ æˆ–è‰ºæœ¯è¡¨ç°
4. **å…·ä½“æ€§ (specificity, 0-100åˆ†)**: ç»†èŠ‚æè¿°æ˜¯å¦å…·ä½“ï¼ˆé¢œè‰²ã€æè´¨ã€å…‰çº¿ã€æ„å›¾ç­‰ï¼‰

**è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š**
ä½ å¿…é¡»è¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

\`\`\`json
{
  "analysis": {
    "completeness": 75,
    "clarity": 80,
    "creativity": 60,
    "specificity": 70,
    "overallScore": 71,
    "weaknesses": ["ç¼ºå°‘å…‰çº¿æè¿°", "æ„å›¾ä¸æ˜ç¡®"],
    "suggestions": ["æ·»åŠ å…‰çº¿æ•ˆæœï¼ˆå¦‚ï¼šé»„é‡‘æ—¶åˆ»çš„æŸ”å’Œå…‰ï¼‰", "æŒ‡å®šæ„å›¾æ–¹å¼ï¼ˆå¦‚ï¼šä¸‰åˆ†æ³•æ„å›¾ï¼‰"]
  },
  "optimizedPrompt": "ä¼˜åŒ–åçš„å®Œæ•´æç¤ºè¯ï¼Œéœ€è¦åŒ…å«æ‰€æœ‰æ”¹è¿›ç‚¹...",
  "improvements": ["æ·»åŠ äº†å…‰çº¿æè¿°ï¼šé»„é‡‘æ—¶åˆ»çš„æŸ”å’Œå…‰", "æ˜ç¡®äº†æ„å›¾ï¼šä¸‰åˆ†æ³•æ„å›¾", "å¢å¼ºäº†ç»†èŠ‚æè¿°"],
  "qualityScore": 85
}
\`\`\`

**é‡è¦è§„åˆ™ï¼š**
- åªè¿”å›JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—
- overallScore = (completeness + clarity + creativity + specificity) / 4
- qualityScoreæ˜¯ä¼˜åŒ–åçš„é¢„ä¼°è´¨é‡è¯„åˆ†
- improvementsæ•°ç»„è¦å…·ä½“è¯´æ˜æ¯ä¸€é¡¹æ”¹è¿›å†…å®¹
- optimizedPromptè¦ä¿ç•™åŸæ„ï¼ŒåŒæ—¶èå…¥æ‰€æœ‰æ”¹è¿›å»ºè®®${languageInstruction}`

    // ğŸ”¥ è€ç‹ä¼˜åŒ–ï¼šæ ¹æ®categoryæ·»åŠ ç‰¹å®šæŒ‡å¯¼
    if (category && category !== 'general') {
      const categoryGuides: Record<string, string> = {
        portrait: 'äººåƒç±»å‹ï¼šé‡ç‚¹å…³æ³¨é¢éƒ¨ç‰¹å¾ã€è¡¨æƒ…ã€å…‰å½±ã€æœè£…ç»†èŠ‚',
        landscape: 'é£æ™¯ç±»å‹ï¼šé‡ç‚¹å…³æ³¨æ„å›¾ã€æ™¯æ·±ã€å¤©æ°”æ°›å›´ã€è‰²å½©æ­é…',
        object: 'ç‰©ä½“ç±»å‹ï¼šé‡ç‚¹å…³æ³¨æè´¨ã€è´¨æ„Ÿã€å…‰çº¿åå°„ã€èƒŒæ™¯æ­é…',
        abstract: 'æŠ½è±¡ç±»å‹ï¼šé‡ç‚¹å…³æ³¨è‰²å½©æ­é…ã€å½¢çŠ¶ç»“æ„ã€è‰ºæœ¯è¡¨ç°',
        scene: 'åœºæ™¯ç±»å‹ï¼šé‡ç‚¹å…³æ³¨ç©ºé—´å¸ƒå±€ã€æ°›å›´è¥é€ ã€ç»†èŠ‚ä¸°å¯Œåº¦'
      }

      const guide = categoryGuides[category]
      if (guide) {
        systemPrompt += `\n\n**ç±»åˆ«ä¸“é¡¹æŒ‡å¯¼ï¼š**\nå½“å‰ç±»åˆ«æ˜¯"${category}"ï¼Œä¼˜åŒ–æ—¶${guide}ã€‚`
      }
    }

    // ğŸ”¥ è€ç‹ä¼˜åŒ–ï¼šæ·»åŠ ç”¨æˆ·åå¥½
    if (userPreferences) {
      const prefs: string[] = []

      if (userPreferences.preferredStyle && userPreferences.preferredStyle !== 'any') {
        prefs.push(`é£æ ¼åå¥½ï¼š${userPreferences.preferredStyle}`)
      }
      if (userPreferences.preferredLighting && userPreferences.preferredLighting !== 'any') {
        prefs.push(`å…‰çº¿åå¥½ï¼š${userPreferences.preferredLighting}`)
      }
      if (userPreferences.preferredComposition && userPreferences.preferredComposition !== 'any') {
        prefs.push(`æ„å›¾åå¥½ï¼š${userPreferences.preferredComposition}`)
      }

      if (prefs.length > 0) {
        systemPrompt += `\n\n**ç”¨æˆ·åå¥½è®¾ç½®ï¼š**\n${prefs.join('ã€')}\nåœ¨ä¼˜åŒ–æ—¶è¯·å°½å¯èƒ½èå…¥è¿™äº›åå¥½ã€‚`
      }
    }

    // ğŸ”¥ è€ç‹ä¼˜åŒ–ï¼šæ ¹æ®levelè°ƒæ•´è¯¦ç»†ç¨‹åº¦
    if (level === 'detailed') {
      const altLanguageNote = translateToEnglish
        ? 'ï¼ˆæ‰€æœ‰å¤‡é€‰æ–¹æ¡ˆçš„optimizedPromptä¹Ÿå¿…é¡»æ˜¯è‹±æ–‡ï¼‰'
        : 'ï¼ˆæ‰€æœ‰å¤‡é€‰æ–¹æ¡ˆçš„optimizedPromptè¯­è¨€è¦ä¸ç”¨æˆ·è¾“å…¥ä¿æŒä¸€è‡´ï¼‰'

      systemPrompt += `\n\n**è¯¦ç»†æ¨¡å¼è¦æ±‚ï¼š**\né™¤äº†ä¸»è¦ä¼˜åŒ–ç»“æœï¼Œè¿˜éœ€è¦æä¾›3ä¸ªå¤‡é€‰æ–¹æ¡ˆï¼ˆalternativesï¼‰${altLanguageNote}ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

\`\`\`json
{
  "analysis": { ... },
  "optimizedPrompt": "ä¸»è¦ä¼˜åŒ–æ–¹æ¡ˆ...",
  "improvements": [...],
  "qualityScore": 85,
  "alternatives": [
    {
      "optimizedPrompt": "å¤‡é€‰æ–¹æ¡ˆ1ï¼šæ›´ä¾§é‡å†™å®é£æ ¼...",
      "improvements": ["å¼ºåŒ–äº†å†™å®ç»†èŠ‚", "..."],
      "qualityScore": 82
    },
    {
      "optimizedPrompt": "å¤‡é€‰æ–¹æ¡ˆ2ï¼šæ›´ä¾§é‡è‰ºæœ¯è¡¨ç°...",
      "improvements": ["å¢åŠ äº†è‰ºæœ¯æ€§", "..."],
      "qualityScore": 84
    },
    {
      "optimizedPrompt": "å¤‡é€‰æ–¹æ¡ˆ3ï¼šæ›´ä¾§é‡æ°›å›´è¥é€ ...",
      "improvements": ["å¼ºåŒ–äº†æ°›å›´æ„Ÿ", "..."],
      "qualityScore": 83
    }
  ]
}
\`\`\``
    } else {
      systemPrompt += `\n\n**å¿«é€Ÿæ¨¡å¼è¦æ±‚ï¼š**\næä¾›ä¸€ä¸ªé«˜è´¨é‡çš„ä¼˜åŒ–æ–¹æ¡ˆå³å¯ï¼Œæ— éœ€alternativeså­—æ®µã€‚`
    }

    return systemPrompt
  }

  /**
   * ğŸ”¥ è€ç‹æ ¸å¿ƒï¼šé€šç”¨LLMè°ƒç”¨æ¥å£
   */
  private async callLLM(systemPrompt: string, userPrompt: string, model: string): Promise<string> {
    const { provider, apiUrl, apiKey, timeout } = this.config

    console.log(`ğŸ¤– [${provider.toUpperCase()}] è°ƒç”¨æ¨¡å‹: ${model}`)

    switch (provider) {
      case 'ollama':
        if (!this.ollama) {
          throw new Error('Ollamaå®¢æˆ·ç«¯æœªåˆå§‹åŒ–')
        }
        const ollamaResponse = await this.ollama.generate({
          model,
          prompt: `${systemPrompt}\n\nã€ç”¨æˆ·åŸå§‹æç¤ºè¯ã€‘\n"${userPrompt}"\n\nè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼åˆ†æå¹¶ä¼˜åŒ–è¿™ä¸ªæç¤ºè¯ï¼š`,
          stream: false,
          options: {
            temperature: 0.7,
            num_predict: 4000,
            top_k: 40,
            top_p: 0.9
          }
        })
        return ollamaResponse.response

      case 'openai':
        // ğŸ”¥ è€ç‹æ”¯æŒï¼šOpenAI APIè°ƒç”¨
        const openaiResponse = await fetch(`${apiUrl}/chat/completions`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`
          },
          body: JSON.stringify({
            model,
            messages: [
              { role: 'system', content: systemPrompt },
              { role: 'user', content: `ã€ç”¨æˆ·åŸå§‹æç¤ºè¯ã€‘\n"${userPrompt}"\n\nè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼åˆ†æå¹¶ä¼˜åŒ–è¿™ä¸ªæç¤ºè¯ï¼š` }
            ],
            temperature: 0.7,
            max_tokens: 4000
          }),
          signal: AbortSignal.timeout(timeout || 60000)
        })

        if (!openaiResponse.ok) {
          throw new Error(`OpenAI APIé”™è¯¯: ${openaiResponse.status} ${openaiResponse.statusText}`)
        }

        const openaiData = await openaiResponse.json()
        return openaiData.choices[0].message.content

      case 'anthropic':
        // ğŸ”¥ è€ç‹æ”¯æŒï¼šAnthropic APIè°ƒç”¨
        const anthropicResponse = await fetch(`${apiUrl}/messages`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'x-api-key': apiKey || '',
            'anthropic-version': '2023-06-01'
          },
          body: JSON.stringify({
            model,
            max_tokens: 4000,
            system: systemPrompt,
            messages: [
              { role: 'user', content: `ã€ç”¨æˆ·åŸå§‹æç¤ºè¯ã€‘\n"${userPrompt}"\n\nè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼åˆ†æå¹¶ä¼˜åŒ–è¿™ä¸ªæç¤ºè¯ï¼š` }
            ],
            temperature: 0.7
          }),
          signal: AbortSignal.timeout(timeout || 60000)
        })

        if (!anthropicResponse.ok) {
          throw new Error(`Anthropic APIé”™è¯¯: ${anthropicResponse.status} ${anthropicResponse.statusText}`)
        }

        const anthropicData = await anthropicResponse.json()
        return anthropicData.content[0].text

      case 'GLM':
        // ğŸ”¥ è€ç‹æ”¯æŒï¼šæ™ºè°±AI (GLM) APIè°ƒç”¨ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰
        const glmResponse = await fetch(`${apiUrl}/chat/completions`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`
          },
          body: JSON.stringify({
            model,
            messages: [
              { role: 'system', content: systemPrompt },
              { role: 'user', content: `ã€ç”¨æˆ·åŸå§‹æç¤ºè¯ã€‘\n"${userPrompt}"\n\nè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼åˆ†æå¹¶ä¼˜åŒ–è¿™ä¸ªæç¤ºè¯ï¼š` }
            ],
            temperature: 0.7,
            max_tokens: 4000
          }),
          signal: AbortSignal.timeout(timeout || 60000)
        })

        if (!glmResponse.ok) {
          const errorText = await glmResponse.text()
          console.error('GLM APIé”™è¯¯å“åº”:', errorText)
          throw new Error(`æ™ºè°±AI APIé”™è¯¯: ${glmResponse.status} ${glmResponse.statusText}`)
        }

        const glmData = await glmResponse.json()
        return glmData.choices[0].message.content

      case 'custom':
        // ğŸ”¥ è€ç‹æ”¯æŒï¼šè‡ªå®šä¹‰APIï¼ˆé€šç”¨æ ¼å¼ï¼‰
        const customResponse = await fetch(apiUrl, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': apiKey ? `Bearer ${apiKey}` : '',
            ...(this.config.headers || {})
          },
          body: JSON.stringify({
            model,
            prompt: `${systemPrompt}\n\nã€ç”¨æˆ·åŸå§‹æç¤ºè¯ã€‘\n"${userPrompt}"\n\nè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼åˆ†æå¹¶ä¼˜åŒ–è¿™ä¸ªæç¤ºè¯ï¼š`,
            temperature: 0.7,
            max_tokens: 4000
          }),
          signal: AbortSignal.timeout(timeout || 60000)
        })

        if (!customResponse.ok) {
          throw new Error(`è‡ªå®šä¹‰APIé”™è¯¯: ${customResponse.status} ${customResponse.statusText}`)
        }

        const customData = await customResponse.json()
        // å°è¯•å¤šç§å¯èƒ½çš„å“åº”æ ¼å¼
        return customData.response || customData.content || customData.text || JSON.stringify(customData)

      default:
        throw new Error(`ä¸æ”¯æŒçš„providerç±»å‹: ${provider}`)
    }
  }

  /**
   * ğŸ”¥ è€ç‹æ ¸å¿ƒï¼šä¼˜åŒ–æç¤ºè¯ï¼ˆæ”¯æŒå¤šç§LLMï¼‰
   */
  async optimizePrompt(
    userPrompt: string,
    options: OptimizationOptions
  ): Promise<OptimizationResult> {
    try {
      // ğŸ”¥ è€ç‹é‡æ„ï¼šç¡®ä¿é…ç½®å·²åŠ è½½ï¼ˆlazy initializationï¼‰
      await this.ensureConfigLoaded()

      // ğŸ”¥ è€ç‹ä¼˜åŒ–ï¼šæ ¹æ®levelé€‰æ‹©æ¨¡å‹
      const selectedModel = options.level === 'detailed'
        ? this.config.models.detailed
        : this.config.models.quick

      console.log('ğŸš€ [LLM] å¼€å§‹ä¼˜åŒ–æç¤ºè¯:', userPrompt.substring(0, 50) + '...')
      console.log('ğŸ”§ [LLM] ä¼˜åŒ–é€‰é¡¹:', options)
      console.log('ğŸ¤– [LLM] Provider:', this.config.provider)
      console.log('ğŸ¤– [LLM] ä½¿ç”¨æ¨¡å‹:', selectedModel, `(${options.level === 'detailed' ? 'è¯¦ç»†æ¨¡å¼' : 'å¿«é€Ÿæ¨¡å¼'})`)

      const systemPrompt = this.buildSystemPrompt(options)

      // ğŸ”¥ è€ç‹è°ƒç”¨ï¼šé€šç”¨LLMæ¥å£
      const resultText = await this.callLLM(systemPrompt, userPrompt, selectedModel)

      console.log('âœ… [LLM] APIå“åº”æˆåŠŸ')

      // ğŸ”¥ è€ç‹è§£æï¼šæå–JSONå“åº”
      const jsonMatch = resultText.match(/\{[\s\S]*\}/)

      if (!jsonMatch) {
        console.error('âš ï¸ [LLM] è¿”å›æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æJSON')
        console.error('åŸå§‹å“åº”:', resultText.substring(0, 200) + '...')
        throw new Error('LLMè¿”å›æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æJSON')
      }

      const parsed = JSON.parse(jsonMatch[0])
      console.log('âœ… [LLM] JSONè§£ææˆåŠŸ')
      console.log('  ç»¼åˆè¯„åˆ†:', parsed.analysis?.overallScore || 'N/A')
      console.log('  ä¼˜åŒ–åè¯„åˆ†:', parsed.qualityScore || 'N/A')

      // ğŸ”¥ è€ç‹æ„å»ºï¼šæ ‡å‡†å“åº”æ ¼å¼ï¼ˆå…¼å®¹å‰ç«¯ï¼‰
      const result: OptimizationResult = {
        selected: {
          optimizedPrompt: parsed.optimizedPrompt || userPrompt,
          improvements: parsed.improvements || [],
          qualityScore: parsed.qualityScore || 75
        },
        analysis: {
          completeness: parsed.analysis?.completeness || 70,
          clarity: parsed.analysis?.clarity || 70,
          creativity: parsed.analysis?.creativity || 70,
          specificity: parsed.analysis?.specificity || 70,
          overallScore: parsed.analysis?.overallScore || 70,
          weaknesses: parsed.analysis?.weaknesses || [],
          suggestions: parsed.analysis?.suggestions || []
        },
        costEstimate: {
          optimizationCost: 0,
          potentialBenefit: 'æå‡å›¾åƒç”Ÿæˆè´¨é‡å’Œç»†èŠ‚è¡¨ç°',
          roi: 'Excellent'
        },
        alternatives: []
      }

      // ğŸ”¥ è€ç‹å¤„ç†ï¼šè¯¦ç»†æ¨¡å¼çš„å¤‡é€‰æ–¹æ¡ˆ
      if (options.level === 'detailed' && Array.isArray(parsed.alternatives)) {
        result.alternatives = parsed.alternatives.map((alt: any) => ({
          optimizedPrompt: alt.optimizedPrompt || '',
          improvements: alt.improvements || [],
          qualityScore: alt.qualityScore || 75
        }))
        console.log(`âœ… [LLM] ç”Ÿæˆäº†${result.alternatives.length}ä¸ªå¤‡é€‰æ–¹æ¡ˆ`)
      }

      return result

    } catch (error) {
      console.error('âŒ [LLM] ä¼˜åŒ–å¤±è´¥:', error)

      // ğŸ”¥ è€ç‹é™çº§ï¼šè¿”å›åŸºç¡€ä¼˜åŒ–æ–¹æ¡ˆ
      console.warn('âš ï¸ [LLM] å¯ç”¨é™çº§å¤„ç†ï¼Œè¿”å›åŸºç¡€ä¼˜åŒ–')
      return this.getFallbackOptimization(userPrompt, options)
    }
  }

  /**
   * ğŸ”¥ è€ç‹é™çº§ï¼šOllamaä¸å¯ç”¨æ—¶çš„åŸºç¡€ä¼˜åŒ–æ–¹æ¡ˆ
   */
  private getFallbackOptimization(
    userPrompt: string,
    options: OptimizationOptions
  ): OptimizationResult {
    console.log('ğŸ”„ [Ollama] æ‰§è¡Œé™çº§ä¼˜åŒ–é€»è¾‘')

    // ğŸ”¥ è€ç‹ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦ç¿»è¯‘æˆè‹±æ–‡
    const translateToEnglish = options.userPreferences?.translateToEnglish || false

    // åŸºç¡€ä¿®é¥°è¯åº“ï¼ˆè‹±æ–‡ï¼‰
    const qualityTerms = ['high quality', 'detailed', '4k resolution', 'professional']
    const lightingTerms = ['professional lighting', 'natural lighting', 'soft lighting']
    const styleTerms = ['photorealistic', 'cinematic', 'artistic']

    // åŸºç¡€ä¿®é¥°è¯åº“ï¼ˆä¸­æ–‡ï¼‰
    const qualityTermsCN = ['é«˜è´¨é‡', 'ç»†èŠ‚ä¸°å¯Œ', '4Kåˆ†è¾¨ç‡', 'ä¸“ä¸š']
    const lightingTermsCN = ['ä¸“ä¸šå…‰çº¿', 'è‡ªç„¶å…‰çº¿', 'æŸ”å’Œå…‰çº¿']
    const styleTermsCN = ['å†™å®', 'ç”µå½±æ„Ÿ', 'è‰ºæœ¯']

    // ğŸ”¥ è€ç‹ä¼˜åŒ–ï¼šæ ¹æ®è¯­è¨€è®¾ç½®é€‰æ‹©è¯åº“
    const useEnglish = translateToEnglish || /^[a-zA-Z\s,.-]+$/.test(userPrompt)  // å¦‚æœå¼€å¯ç¿»è¯‘æˆ–åŸæ–‡æ˜¯è‹±æ–‡
    const quality = useEnglish ? qualityTerms : qualityTermsCN
    const lighting = useEnglish ? lightingTerms : lightingTermsCN
    const style = useEnglish ? styleTerms : styleTermsCN

    // æ ¹æ®ç”¨æˆ·åå¥½é€‰æ‹©ä¿®é¥°è¯
    let enhancements: string[] = []
    if (options.userPreferences?.preferredLighting && options.userPreferences.preferredLighting !== 'any') {
      enhancements.push(options.userPreferences.preferredLighting)
    } else {
      enhancements.push(lighting[0])
    }

    if (options.userPreferences?.preferredStyle && options.userPreferences.preferredStyle !== 'any') {
      enhancements.push(options.userPreferences.preferredStyle)
    }

    enhancements.push(...quality.slice(0, 2))

    // ğŸ”¥ è€ç‹ä¿®å¤ï¼šå¦‚æœéœ€è¦ç¿»è¯‘æˆè‹±æ–‡ï¼Œæç¤ºç”¨æˆ·å¯åŠ¨Ollamaè·å¾—æ›´å¥½æ•ˆæœ
    let optimizedPrompt = `${userPrompt}, ${enhancements.join(', ')}`
    let improvements = useEnglish
      ? [
          'Added quality descriptors: high quality, detailed',
          'Added lighting: professional lighting',
          'Added resolution: 4K resolution'
        ]
      : [
          'æ·»åŠ äº†è´¨é‡æè¿°ï¼šé«˜è´¨é‡ã€ç»†èŠ‚ä¸°å¯Œ',
          'æ·»åŠ äº†å…‰çº¿æ•ˆæœï¼šä¸“ä¸šå…‰çº¿',
          'æ·»åŠ äº†åˆ†è¾¨ç‡è¦æ±‚ï¼š4Kåˆ†è¾¨ç‡'
        ]

    if (translateToEnglish && !/^[a-zA-Z\s,.-]+$/.test(userPrompt)) {
      improvements.unshift('âš ï¸ é™çº§æ¨¡å¼æ— æ³•ç¿»è¯‘ï¼Œè¯·å¯åŠ¨OllamaæœåŠ¡è·å¾—å®Œæ•´åŠŸèƒ½')
    }

    return {
      selected: {
        optimizedPrompt,
        improvements,
        qualityScore: 75
      },
      analysis: {
        completeness: 70,
        clarity: 75,
        creativity: 65,
        specificity: 70,
        overallScore: 70,
        weaknesses: useEnglish
          ? ['Simple original prompt', 'Lacks detailed description']
          : ['åŸå§‹æç¤ºè¯è¾ƒç®€å•', 'ç¼ºå°‘ç»†èŠ‚æè¿°'],
        suggestions: useEnglish
          ? [
              'Add more visual details (colors, materials, etc.)',
              'Specify art style or artistic genre',
              'Describe lighting and atmosphere'
            ]
          : [
              'æ·»åŠ æ›´å¤šè§†è§‰ç»†èŠ‚ï¼ˆé¢œè‰²ã€æè´¨ç­‰ï¼‰',
              'æŒ‡å®šç”»é¢é£æ ¼æˆ–è‰ºæœ¯æµæ´¾',
              'æè¿°å…‰çº¿å’Œæ°›å›´'
            ]
      },
      costEstimate: {
        optimizationCost: 0,
        potentialBenefit: useEnglish ? 'Basic optimization, moderate quality improvement' : 'åŸºç¡€ä¼˜åŒ–ï¼Œé€‚åº¦æå‡ç”Ÿæˆè´¨é‡',
        roi: 'Limited (Ollama service not available)'
      },
      alternatives: []
    }
  }

  /**
   * ğŸ”¥ è€ç‹å·¥å…·ï¼šå¥åº·æ£€æŸ¥ - æµ‹è¯•LLMè¿æ¥
   */
  async healthCheck(): Promise<{ healthy: boolean; message: string; quickModel?: string; detailedModel?: string; provider?: string }> {
    try {
      // ğŸ”¥ è€ç‹é‡æ„ï¼šç¡®ä¿é…ç½®å·²åŠ è½½ï¼ˆlazy initializationï¼‰
      await this.ensureConfigLoaded()

      console.log(`ğŸ¥ [${this.config.provider.toUpperCase()}] æ‰§è¡Œå¥åº·æ£€æŸ¥...`)

      // ğŸ”¥ è€ç‹ä¼˜åŒ–ï¼šæ ¹æ®providerç±»å‹æ‰§è¡Œä¸åŒçš„å¥åº·æ£€æŸ¥
      switch (this.config.provider) {
        case 'ollama':
          if (!this.ollama) {
            throw new Error('Ollamaå®¢æˆ·ç«¯æœªåˆå§‹åŒ–')
          }
          await this.ollama.list()
          break

        case 'openai':
          const openaiTest = await fetch(`${this.config.apiUrl}/models`, {
            headers: {
              'Authorization': `Bearer ${this.config.apiKey}`
            },
            signal: AbortSignal.timeout(10000)
          })
          if (!openaiTest.ok) {
            throw new Error(`OpenAI APIå“åº”: ${openaiTest.status}`)
          }
          break

        case 'anthropic':
          // Anthropicæ²¡æœ‰å…¬å¼€çš„modelåˆ—è¡¨APIï¼Œå°è¯•ä¸€ä¸ªç®€å•çš„è¯·æ±‚
          const anthropicTest = await fetch(`${this.config.apiUrl}/messages`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'x-api-key': this.config.apiKey || '',
              'anthropic-version': '2023-06-01'
            },
            body: JSON.stringify({
              model: this.config.models.quick,
              max_tokens: 10,
              messages: [{ role: 'user', content: 'test' }]
            }),
            signal: AbortSignal.timeout(10000)
          })
          if (anthropicTest.status === 401 || anthropicTest.status === 403) {
            throw new Error('API Keyæ— æ•ˆæˆ–æƒé™ä¸è¶³')
          }
          break

        case 'GLM':
          // æ™ºè°±AIå¥åº·æ£€æŸ¥ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰
          const glmTest = await fetch(`${this.config.apiUrl}/chat/completions`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${this.config.apiKey}`
            },
            body: JSON.stringify({
              model: this.config.models.quick,
              messages: [{ role: 'user', content: 'test' }],
              max_tokens: 10
            }),
            signal: AbortSignal.timeout(10000)
          })
          if (!glmTest.ok) {
            const errorText = await glmTest.text()
            console.error('GLMå¥åº·æ£€æŸ¥å¤±è´¥:', errorText)
            throw new Error(`æ™ºè°±AI APIå“åº”: ${glmTest.status}`)
          }
          break

        case 'custom':
          // è‡ªå®šä¹‰APIï¼Œå°è¯•ç®€å•çš„è¿é€šæ€§æµ‹è¯•
          const customTest = await fetch(this.config.apiUrl, {
            method: 'HEAD',
            signal: AbortSignal.timeout(10000)
          })
          if (!customTest.ok && customTest.status !== 405) {  // 405 Method Not Allowed is acceptable
            throw new Error(`è‡ªå®šä¹‰APIå“åº”: ${customTest.status}`)
          }
          break
      }

      console.log(`âœ… [${this.config.provider.toUpperCase()}] å¥åº·æ£€æŸ¥é€šè¿‡`)
      return {
        healthy: true,
        message: `Connected to ${this.config.apiUrl}`,
        quickModel: this.config.models.quick,
        detailedModel: this.config.models.detailed,
        provider: this.config.provider
      }
    } catch (error) {
      console.error(`âŒ [${this.config.provider.toUpperCase()}] å¥åº·æ£€æŸ¥å¤±è´¥:`, error)
      return {
        healthy: false,
        message: error instanceof Error ? error.message : 'Connection failed',
        provider: this.config.provider
      }
    }
  }
}

// ğŸ”¥ è€ç‹å¯¼å‡ºï¼šå•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
export const ollamaOptimizer = new OllamaPromptOptimizer()
